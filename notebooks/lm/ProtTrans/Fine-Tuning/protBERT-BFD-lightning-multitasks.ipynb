{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ProtTrans (Protein Transformer)\n\nFrom https://github.com/agemagician/ProtTrans : ProtTrans is providing state of the art pre-trained models for proteins. ProtTrans was trained on thousands of GPUs from Summit and hundreds of Google TPUs using various Transformer models.\n\nHave a look at our paper [ProtTrans: cracking the language of lifeâ€™s code through self-supervised deep learning and high performance computing](https://doi.org/10.1109/TPAMI.2021.3095381) for more information about this work.\n\n## Lightning's ProtBERT multi-task classifier\n**This notebook llustrates how to make a \"multi-task\" classifier using ProtBERT with Pytorch-lightning**. This notebook is an updated version of offical notebook [here](https://github.com/agemagician/ProtTrans/tree/master/Fine-Tuning). Compared to the original notebook, this notebook\n\n* uses up-to-date API of Huggingface's transformer (4.18.0) and Pytorch lightning (1.6.0)\n* uses the new bug-fixing dataset (details here: https://github.com/agemagician/ProtTrans/issues/74 )\n* supports multi-task automatically (by detecting the columns in the input csv)\n* supports wandb logger with Kaggle's secret adds-on so that you can compare various experimental results\n* uses Kaggle's P100 free GPU so no need to subscribe Colab-pro :)\n\n## RAM warning\nKaggle notebook with GPU has just 13GB RAM. With max protein length of 512, the notebook will use most of the RAM, but sometimes it will call out for more RAM and make the notebook crash. We could reduce the protein length if you want just to test run the program.\n\n![](https://github.com/agemagician/ProtTrans/raw/master/images/transformers_attention.png)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T07:16:24.671965Z","iopub.execute_input":"2022-05-07T07:16:24.672541Z","iopub.status.idle":"2022-05-07T07:16:24.690478Z","shell.execute_reply.started":"2022-05-07T07:16:24.672451Z","shell.execute_reply":"2022-05-07T07:16:24.68979Z"}}},{"cell_type":"code","source":"# Model's hyperparameters\nBATCH_SIZE = 1\nACCUM_BATCH = 32\nMAX_PROTEIN_STR_LEN = 512\nENCODER_LR = 5e-6\nGENERAL_LR = 3e-05\nN_FROZEN_EPOCH = 1\nGRADIENT_CHECKPOINT = False # True if RAM < 16GB\nSEED = 43\n\n# File's hyperparameters\nFILE_DIR = \"/kaggle/working/data/\" \nCOL_NAMES = ['input','loc','membrane'] # optional, but no need\n\n# Trainer's hyperparameters\nCPU_WORKERS = 2\nN_CHECKPOINTS = 1 # number of top-k models you want to save\nMONITOR_METRIC = \"val_loss\"\nMONITOR_MODE = \"min\"\nMIN_EPOCHS = 1\nMAX_EPOCHS = 7 # Increase to 10-20 epochs to maximize accuracy (needs more GPU hours on Kaggle)\nPATIENCE = MAX_EPOCHS\nNUM_GPU = 1\nPRECISION = 32\nAMP_BACKEND = \"native\"\n\nwandb_flag=True # Need a free wandb.ai account (retrieve a personal key there)\nwandb_name=\"Run 4. %d-maxlen\" % MAX_PROTEIN_STR_LEN\nwandb_project=\"ProtBERT multi-task DeepLoc\"\n\nprint(wandb_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:46:52.000769Z","iopub.execute_input":"2022-05-09T06:46:52.00145Z","iopub.status.idle":"2022-05-09T06:46:52.111606Z","shell.execute_reply.started":"2022-05-09T06:46:52.001347Z","shell.execute_reply":"2022-05-09T06:46:52.110268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optional: for APEX mixed precision, but now Pytorch has a native mixed precision support already\nif PRECISION==16:\n    print('installing Apex...')\n    !git clone https://github.com/NVIDIA/apex\n    !cd apex && pip install -v --disable-pip-version-check --no-cache-dir ./\n    AMP_BACKEND = 'apex'\n    \n!pip uninstall -y torchtext\n!pip install -q transformers==4.18.0 pytorch-lightning==1.6.0\n!pip install -q pytorch-nlp torchmetrics \n\nprint(AMP_BACKEND)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-09T06:46:52.114791Z","iopub.execute_input":"2022-05-09T06:46:52.11531Z","iopub.status.idle":"2022-05-09T06:47:15.264054Z","shell.execute_reply.started":"2022-05-09T06:46:52.11525Z","shell.execute_reply":"2022-05-09T06:47:15.263014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader, RandomSampler\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom pytorch_lightning import Trainer, seed_everything\nfrom torchmetrics import Accuracy\n\nfrom torchnlp.encoders import LabelEncoder\nfrom torchnlp.datasets.dataset import Dataset\nfrom torchnlp.utils import collate_tensors\n\nimport pandas as pd\nfrom argparse import ArgumentParser\nimport os\nimport re\nimport requests\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom collections import OrderedDict\nimport logging as log\nimport numpy as np\nimport glob","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:15.267267Z","iopub.execute_input":"2022-05-09T06:47:15.267642Z","iopub.status.idle":"2022-05-09T06:47:18.026022Z","shell.execute_reply.started":"2022-05-09T06:47:15.267596Z","shell.execute_reply":"2022-05-09T06:47:18.024836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom IPython.display import display\nprint(torch.__version__)\nprint(pl.__version__)\nprint(transformers.__version__)\n\nHyperOptArgumentParser = ArgumentParser","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:18.030558Z","iopub.execute_input":"2022-05-09T06:47:18.031801Z","iopub.status.idle":"2022-05-09T06:47:22.044884Z","shell.execute_reply.started":"2022-05-09T06:47:18.031753Z","shell.execute_reply":"2022-05-09T06:47:22.043783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n\nif wandb_flag:\n    os.environ[\"WANDB_API_KEY\"] = secret_value_0\n    import wandb\n    from pytorch_lightning.loggers import WandbLogger\n    wandb_logger = WandbLogger(name=wandb_name,project=wandb_project)\n\n    print('wandb is running!')\nelse:\n    wandb_logger = None","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:22.04695Z","iopub.execute_input":"2022-05-09T06:47:22.047594Z","iopub.status.idle":"2022-05-09T06:47:22.37867Z","shell.execute_reply.started":"2022-05-09T06:47:22.047547Z","shell.execute_reply":"2022-05-09T06:47:22.377619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Loc_dataset():\n    \"\"\"\n    Loads the Dataset from the csv files passed to the parser.\n    :param hparam: HyperOptArgumentParser obj containg the path to the data files.\n    :param train: flag to return the train set.\n    :param val: flag to return the validation set.\n    :param test: flag to return the test set.\n    Returns:\n        - Training Dataset, Development Dataset, Testing Dataset\n    \"\"\"\n    def  __init__(self, col_names = None) -> None:\n        self.downloadDeeplocDataset()\n        self.col_names = col_names\n        \n    def downloadDeeplocDataset(self):\n        # The old dataset is obsoleted as discussed in https://github.com/agemagician/ProtTrans/issues/74\n#         deeplocDatasetTrainUrl = 'https://www.dropbox.com/s/vgdqcl4vzqm9as0/deeploc_per_protein_train.csv?dl=1'\n#         deeplocDatasetValidUrl = 'https://www.dropbox.com/s/jfzuokrym7nflkp/deeploc_per_protein_test.csv?dl=1'\n        deeplocDatasetTrainUrl = 'https://rostlab.org/~deepppi/deeploc_data/deeploc_our_train_set.csv'\n        deeplocDatasetValidUrl = 'https://rostlab.org/~deepppi/deeploc_data/deeploc_our_val_set.csv'\n        deeplocDatasetTestUrl = 'https://rostlab.org/~deepppi/deeploc_data/deeploc_test_set.csv'\n        deeplocDatasetHardUrl = 'https://rostlab.org/~deepppi/deeploc_data/setHARD.csv'\n    \n        datasetFolderPath = 'data/'\n        self.trainFilePath = os.path.join(datasetFolderPath, 'deeploc_per_protein_train.csv')\n        self.validFilePath = os.path.join(datasetFolderPath, 'deeploc_per_protein_valid.csv')\n        self.testFilePath = os.path.join(datasetFolderPath, 'deeploc_per_protein_test.csv')\n        self.hardFilePath = os.path.join(datasetFolderPath, 'deeploc_per_protein_hard.csv')\n\n\n        if not os.path.exists(datasetFolderPath):\n            os.makedirs(datasetFolderPath)\n\n        def download_file(url, filename):\n            response = requests.get(url, stream=True)\n            with tqdm.wrapattr(open(filename, \"wb\"), \"write\", miniters=1,\n                              total=int(response.headers.get('content-length', 0)),\n                              desc=filename) as fout:\n                for chunk in response.iter_content(chunk_size=4096):\n                    fout.write(chunk)\n\n        if not os.path.exists(self.trainFilePath):\n            download_file(deeplocDatasetTrainUrl, self.trainFilePath)\n\n        if not os.path.exists(self.testFilePath):\n            download_file(deeplocDatasetTestUrl, self.testFilePath)\n        \n        if not os.path.exists(self.validFilePath):\n            download_file(deeplocDatasetValidUrl, self.validFilePath)\n\n        if not os.path.exists(self.hardFilePath):\n            download_file(deeplocDatasetHardUrl, self.hardFilePath)\n            \n    def collate_lists(self, seq: list, label: list) -> dict:\n        \"\"\" Converts each line into a dictionary. \"\"\"\n        collated_dataset = []\n        for i in range(len(seq)):\n            collated_dataset.append({\"seq\": str(seq[i]), \"label\": str(label[i]).split()})\n        return collated_dataset\n    \n    def protein_df_stat(self, df):\n        print(df.shape)\n        all_strings = []\n        for i in tqdm(range(len(df))):\n            all_strings += df[df.columns[0]].iloc[i].split()\n        print('total alphabets', len(all_strings))\n        print()\n    \n        all_alphabets = np.unique(all_strings)\n        print(all_alphabets, len(all_alphabets))\n        print()\n    \n        all_strings_np = np.array(all_strings)\n        num_alphabets_dict = OrderedDict() \n        for a in all_alphabets:\n            num_alphabets_dict[a] = (all_strings_np==a).sum()\n            print(a, num_alphabets_dict[a])\n            \n        tasks_list = df.columns[1:].values\n        tasks_info_dict = OrderedDict()\n        for t in tasks_list:\n            tasks_info_dict[t] = {\"n_labels\": len(df[t].unique()),\n                                  \"label_names\": df[t].unique()}\n        print(tasks_info_dict)\n    \n    def load_df(self, df_name='train', max_len=MAX_PROTEIN_STR_LEN, print_stat=False):\n        if df_name=='train':\n            path = self.trainFilePath\n        elif df_name=='valid' or df_name=='val':\n            path = self.validFilePath\n        elif df_name=='hard':\n            path = self.hardFilePath\n        else:\n            path = self.testFilePath\n        \n        # assume no columns' names are given in csv file\n        # otherwise use pd.read_csv(path,names=col_names,skiprows=0) instead\n        df = pd.read_csv(path)\n\n        if self.col_names is not None: \n            assert len(self.col_names) == len(df.columns)\n        else:\n            self.col_names = ['input']\n            self.col_names += ['task_%d' % i for i in range(len(df.columns[1:]))]\n        df.columns = self.col_names\n            \n        df2 = df.copy()\n        df2['len'] = df[df.columns[0]].apply(lambda x: len(x.split()))\n        \n        if print_stat:\n            print('original df')\n            self.protein_df_stat(df)\n            \n        df = df[df2.len <= max_len]\n        \n        if print_stat:\n            print('truncated df')\n            self.protein_df_stat(df)\n        \n        return df\n    \n    def load_dataset(self,df_name='train', max_len=MAX_PROTEIN_STR_LEN, print_stat=False):\n        \n        df = self.load_df(df_name=df_name, max_len=max_len, print_stat=print_stat)\n        \n        seq = list(df[df.columns[0]])\n        \n        tasks_list = df.columns[1:].values\n        label = []\n        for i in range(len(df)):\n            label_i_str = \" \".join([df[t].values[i] for t in tasks_list])\n            label.append(label_i_str)\n    \n        # Make sure there is a space between every token, and map rarely amino acids\n        seq = [\" \".join(\"\".join(sample.split())) for sample in seq]\n        seq = [re.sub(r\"[UZOB]\", \"X\", sample) for sample in seq]\n        \n#         print(len(seq), len(label), label[:10])\n        assert len(seq) == len(label)\n        return Dataset(self.collate_lists(seq, label))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:22.384007Z","iopub.execute_input":"2022-05-09T06:47:22.386781Z","iopub.status.idle":"2022-05-09T06:47:22.443245Z","shell.execute_reply.started":"2022-05-09T06:47:22.386734Z","shell.execute_reply":"2022-05-09T06:47:22.442018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = Loc_dataset()\ntrain_dataset = dataset.load_dataset('train')\ntest_dataset = dataset.load_dataset('test')\nvalid_dataset = dataset.load_dataset('val')\nhard_dataset = dataset.load_dataset('hard')","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:22.449224Z","iopub.execute_input":"2022-05-09T06:47:22.452316Z","iopub.status.idle":"2022-05-09T06:47:23.283481Z","shell.execute_reply.started":"2022-05-09T06:47:22.45227Z","shell.execute_reply":"2022-05-09T06:47:23.282497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use print_stat = True to see dataset's basic statistics (see example below)\ntrain_df = dataset.load_df('train', print_stat=False)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:23.285433Z","iopub.execute_input":"2022-05-09T06:47:23.285816Z","iopub.status.idle":"2022-05-09T06:47:23.503372Z","shell.execute_reply.started":"2022-05-09T06:47:23.285772Z","shell.execute_reply":"2022-05-09T06:47:23.502165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = dataset.load_df('test', print_stat=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:23.505618Z","iopub.execute_input":"2022-05-09T06:47:23.506032Z","iopub.status.idle":"2022-05-09T06:47:25.275042Z","shell.execute_reply.started":"2022-05-09T06:47:23.505973Z","shell.execute_reply":"2022-05-09T06:47:25.274014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Create the ProtBert pytorch lighting class","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\n\nclass ProtBertBFDClassifier(pl.LightningModule):\n    \"\"\"\n    # https://github.com/minimalist-nlp/lightning-text-classification.git\n    \n    Sample model to show how to use BERT to classify sentences.\n    \n    :param hparam: ArgumentParser containing the hyperparameters.\n    \"\"\"\n\n    def __init__(self, hparam) -> None:\n        super(ProtBertBFDClassifier, self).__init__()\n        self.hparam = hparam\n        self.batch_size = self.hparam.batch_size\n\n        self.model_name = \"Rostlab/prot_bert_bfd\"\n        \n        # TOFIX:\n        self.dataset = Loc_dataset(col_names=COL_NAMES) #self.hparam.col_names)\n        self.metric_acc = Accuracy()\n\n        # build multi-tasks model -> need (multi) tasks_info\n        self.tasks_info_dict = self.extract_tasks_info()\n        \n        self.label_encoders_dict = self.create_label_encoders()\n        self.__build_model()\n\n        # Loss criterion initialization.\n        self.__build_loss()\n\n        if self.hparam.nr_frozen_epochs > 0:\n            self.freeze_encoder()\n        else:\n            self._frozen = False\n        self.nr_frozen_epochs = self.hparam.nr_frozen_epochs\n    \n    def create_label_encoders(self) -> OrderedDict:\n        label_encoders_dict = OrderedDict()\n        \n        for task in self.tasks_info_dict.keys():\n            label_encoders_dict[task] = LabelEncoder(self.tasks_info_dict[task][\"label_names\"], \n                                                  reserved_labels=[], \n                                                  unknown_index=None)\n        return label_encoders_dict\n        \n    def extract_tasks_info(self) -> OrderedDict:\n#         tasks_df = pd.read_csv(self.hparam.train_csv)\n        \n        tasks_df = self.dataset.load_df('train')\n        tasks_list = tasks_df.columns[1:].values # ALWAYS assuming the first column is about input, not task\n        tasks_list = [t.strip() for t in tasks_list]\n        tasks_info_dict = OrderedDict()\n        for t in tasks_list:\n            tasks_info_dict[t] = {\"n_labels\": len(tasks_df[t].unique()),\n                                  \"label_names\": tasks_df[t].unique()}\n        return tasks_info_dict\n        \n    def __build_model(self) -> None:\n        \"\"\" Init BERT model + tokenizer + classification head.\"\"\"\n        self.ProtBertBFD = BertModel.from_pretrained(self.model_name,\n                                                    #  gradient_checkpointing=self.hparam.gradient_checkpointing\n                                                     )\n        if self.hparam.gradient_checkpointing:\n            self.ProtBertBFD.gradient_checkpointing_enable() # HF >= 4.17\n        self.encoder_features = 1024\n        self.pooled_encoder_features = self.encoder_features*4 # 4 pooling strategies by default\n\n        # Tokenizer\n        self.tokenizer = BertTokenizer.from_pretrained(self.model_name, do_lower_case=False)\n\n        # Classification head, one for each task\n        self.classification_heads_dict = nn.ModuleDict()\n        for i,task in enumerate(self.tasks_info_dict.keys()):\n            self.classification_heads_dict[task] = nn.Sequential(nn.Linear(self.pooled_encoder_features, \n                                                                        self.label_encoders_dict[task].vocab_size),\n                                                              nn.Tanh())\n\n            # NOTE: as lighting does not auto transfer nn.ModuleDict()/ModuleList() to cuda()\n            self.classification_heads_dict[task] = self.classification_heads_dict[task].to(self.ProtBertBFD.device)\n\n    def __build_loss(self) -> None:\n        \"\"\" Initializes the loss function/s. \"\"\"\n        self.task_loss_functions_dict = OrderedDict()\n        for t in self.tasks_info_dict.keys():\n            # TODO: add customization\n            self.task_loss_functions_dict[t] = nn.CrossEntropyLoss()\n\n    def unfreeze_encoder(self) -> None:\n        \"\"\" un-freezes the encoder layer. \"\"\"\n        if self._frozen:\n            log.info(f\"\\n-- Encoder model fine-tuning\")\n            for param in self.ProtBertBFD.parameters():\n                param.requires_grad = True\n            self._frozen = False\n\n    def freeze_encoder(self) -> None:\n        \"\"\" freezes the encoder layer. \"\"\"\n        for param in self.ProtBertBFD.parameters():\n            param.requires_grad = False\n        self._frozen = True\n\n    def predict(self, sample: dict) -> dict:\n        \"\"\" Predict function.\n        :param sample: dictionary with the text we want to classify.\n        Returns:\n            Dictionary with the input text and the predicted label.\n        \"\"\"\n        if self.training:\n            self.eval()\n\n        with torch.no_grad():\n            model_input, _ = self.prepare_sample([sample], prepare_target=False)\n            task_predictions_dict = self.forward(**model_input)\n            predicted_labels_dict = OrderedDict()\n            for t in self.tasks_info_dict.keys():\n                logits = task_predictions_dict[t][\"logits\"].numpy()\n                predicted_labels_dict[t] = [self.label_encoders_dict[t].index_to_token[prediction]\n                                            for prediction in np.argmax(logits, axis=1)\n                                           ][0]\n            sample[\"predicted_label\"] = predicted_labels_dict\n\n        return sample\n    \n    # https://github.com/UKPLab/sentence-transformers/blob/eb39d0199508149b9d32c1677ee9953a84757ae4/sentence_transformers/models/Pooling.py\n    def pool_strategy(self, features,\n                      pool_cls=True, pool_max=True, pool_mean=True,\n                      pool_mean_sqrt=True):\n        token_embeddings = features['token_embeddings']\n        cls_token = features['cls_token_embeddings']\n        attention_mask = features['attention_mask']\n\n        ## Pooling strategy\n        output_vectors = []\n        if pool_cls:\n            output_vectors.append(cls_token)\n        if pool_max:\n            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n            token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n            max_over_time = torch.max(token_embeddings, 1)[0]\n            output_vectors.append(max_over_time)\n        if pool_mean or pool_mean_sqrt:\n            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n\n            #If tokens are weighted (by WordWeights layer), feature 'token_weights_sum' will be present\n            if 'token_weights_sum' in features:\n                sum_mask = features['token_weights_sum'].unsqueeze(-1).expand(sum_embeddings.size())\n            else:\n                sum_mask = input_mask_expanded.sum(1)\n\n            sum_mask = torch.clamp(sum_mask, min=1e-9)\n\n            if pool_mean:\n                output_vectors.append(sum_embeddings / sum_mask)\n            if pool_mean_sqrt:\n                output_vectors.append(sum_embeddings / torch.sqrt(sum_mask))\n\n        output_vector = torch.cat(output_vectors, 1)\n        return output_vector\n    \n    def forward(self, input_ids, token_type_ids, attention_mask):\n        \"\"\" Usual pytorch forward function.\n        :param tokens: text sequences [batch_size x src_seq_len]\n        :param lengths: source lengths [batch_size]\n        Returns:\n            Dictionary with model outputs (e.g: logits)\n        \"\"\"\n        input_ids = torch.tensor(input_ids, device=self.device)\n        attention_mask = torch.tensor(attention_mask,device=self.device)\n\n        word_embeddings = self.ProtBertBFD(input_ids,\n                                           attention_mask)[0]\n\n        pooling = self.pool_strategy({\"token_embeddings\": word_embeddings,\n                                      \"cls_token_embeddings\": word_embeddings[:, 0],\n                                      \"attention_mask\": attention_mask,\n                                      })\n        \n        \n        \n        task_predictions_dict = OrderedDict()\n        for i,t in enumerate(self.tasks_info_dict.keys()):\n            task_predictions_dict[t] = {\"logits\": self.classification_heads_dict[t](pooling)}\n        return task_predictions_dict\n\n    def compute_loss(self, predictions: dict, targets: dict) -> torch.tensor:\n        \"\"\"\n        Computes Loss value according to a loss function.\n        :param predictions: model specific output. Must contain a key 'logits' with\n            a tensor [batch_size x 1] with model predictions\n        :param labels: Label values [batch_size]\n        Returns:\n            torch.tensor with loss value.\n        \"\"\"\n        \n        self.task_losses_dict = OrderedDict()\n        loss = torch.tensor(0.0, dtype=torch.float32, requires_grad=True).to(self.ProtBertBFD.device)\n        for i,t in enumerate(self.tasks_info_dict.keys()):\n            self.task_losses_dict[t] = self.task_loss_functions_dict[t](predictions[t][\"logits\"], targets[t][\"labels\"])\n            loss += self.task_losses_dict[t]\n        \n        return loss \n\n    def prepare_sample(self, sample: list, prepare_target: bool = True) -> (dict, dict):\n        \"\"\"\n        Function that prepares a sample to input the model.\n        :param sample: list of dictionaries.\n        \n        Returns:\n            - dictionary with the expected model inputs.\n            - dictionary with the expected target labels.\n        \"\"\"\n        # https://pytorchnlp.readthedocs.io/en/latest/source/torchnlp.utils.html # collate_tensors\n        sample = collate_tensors(sample)\n\n        inputs = self.tokenizer.batch_encode_plus(sample[\"seq\"],\n                                                  add_special_tokens=True,\n                                                  padding=True,\n                                                  truncation=True,\n                                                  max_length=self.hparam.max_length)\n\n        if not prepare_target:\n            return inputs, {}\n\n        # Prepare target:\n        try:\n            targets = OrderedDict()\n            for i,t in enumerate(self.tasks_info_dict.keys()):\n                # i and t will corresponded by construction of ordered_dict\n                targets[t] = {\"labels\": self.label_encoders_dict[t].batch_encode(sample[\"label\"][i])}\n            return inputs, targets\n        except RuntimeError:\n            print(sample[\"label\"])\n            raise Exception(\"Label encoder found an unknown label.\")\n\n    def training_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n        \"\"\" \n        Runs one training step. This usually consists in the forward function followed\n            by the loss function.\n        \n        :param batch: The output of your dataloader. \n        :param batch_nb: Integer displaying which batch this is\n        Returns:\n            - dictionary containing the loss and the metrics to be added to the lightning logger.\n        \"\"\"\n        inputs, targets = batch\n        model_out = self.forward(**inputs)\n        loss_train = self.compute_loss(model_out, targets)\n\n        output = OrderedDict(\n            {\"loss\": loss_train})\n        \n        for t in self.tasks_info_dict.keys():\n            y = targets[t][\"labels\"]\n            y_hat = model_out[t][\"logits\"]\n        \n            labels_hat = torch.argmax(y_hat, dim=1)\n            train_acc = self.metric_acc(labels_hat, y)\n            output[\"train_acc_\"+ t] = train_acc\n            output[\"train_loss_\"+ t] = self.task_losses_dict[t]\n            \n        self.log_dict(output, prog_bar=True)\n        return output\n\n    def validation_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n        \"\"\" Similar to the training step but with the model in eval mode.\n        Returns:\n            - dictionary passed to the validation_end function.\n        \"\"\"\n        inputs, targets = batch\n\n        model_out = self.forward(**inputs)\n        loss_val = self.compute_loss(model_out, targets)\n        \n        output = OrderedDict({\"val_loss\": loss_val})\n        for t in self.tasks_info_dict.keys():\n            y = targets[t][\"labels\"]\n            y_hat = model_out[t][\"logits\"]\n        \n            labels_hat = torch.argmax(y_hat, dim=1)\n            val_acc = self.metric_acc(labels_hat, y)\n            output[\"val_acc_\"+ t] = val_acc\n        \n        self.log_dict(output)\n        \n        return output\n    \n\n    def test_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n        \"\"\" Similar to the training step but with the model in eval mode.\n        Returns:\n            - dictionary passed to the validation_end function.\n        \"\"\"\n        inputs, targets = batch\n        model_out = self.forward(**inputs)\n        loss_test = self.compute_loss(model_out, targets)\n\n        output = OrderedDict({\"test_loss\": loss_test})\n        for t in self.tasks_info_dict.keys():\n            y = targets[t][\"labels\"]\n            y_hat = model_out[t][\"logits\"]\n        \n            labels_hat = torch.argmax(y_hat, dim=1)\n            test_acc = self.metric_acc(labels_hat, y)\n            output[\"test_acc_\"+ t] = test_acc\n            \n        self.log_dict(output)\n        return output\n\n    def configure_optimizers(self):\n        \"\"\" Sets different Learning rates for different parameter groups. \"\"\"\n        parameters = [{\"params\": self.classification_heads_dict[t].parameters()}\n                                 for i,t in enumerate(self.tasks_info_dict.keys())]\n        \n        parameters += [{\"params\": self.ProtBertBFD.parameters(),\n                        \"lr\": self.hparam.encoder_learning_rate,}]\n        \n        optimizer = optim.Adam(parameters, lr=self.hparam.learning_rate)\n        return [optimizer], []\n\n    def on_train_epoch_end(self):\n        \"\"\" Pytorch lightning hook \"\"\"\n        if self.current_epoch + 1 >= self.nr_frozen_epochs:\n            self.unfreeze_encoder()\n            if self.current_epoch + 1 == self.nr_frozen_epochs:\n                print('\\n Unfreeze the encoder!! \\n')\n\n    def __retrieve_dataset(self, train=True, val=True, test=True):\n        \"\"\" Retrieves task specific dataset \"\"\"\n        \n        if train:\n            return self.dataset.load_dataset('train')#[:64] # in case you want a quick test\n        elif val:\n            return self.dataset.load_dataset('val')#[:64]\n        elif test:\n            return self.dataset.load_dataset('test')\n        else:\n            print('hard dataset')\n            return self.dataset.load_dataset('hard')\n\n    def train_dataloader(self) -> DataLoader:\n        \"\"\" Function that loads the train set. \"\"\"\n        self._train_dataset = self.__retrieve_dataset(val=False, test=False)\n        return DataLoader(\n            dataset=self._train_dataset,\n            sampler=RandomSampler(self._train_dataset),\n            batch_size=self.hparam.batch_size,\n            collate_fn=self.prepare_sample,\n            num_workers=self.hparam.loader_workers,\n        )\n\n    def val_dataloader(self) -> DataLoader:\n        \"\"\" Function that loads the validation set. \"\"\"\n        self._dev_dataset = self.__retrieve_dataset(train=False, test=False)\n        return DataLoader(\n            dataset=self._dev_dataset,\n            batch_size=self.hparam.batch_size,\n            collate_fn=self.prepare_sample,\n            num_workers=self.hparam.loader_workers,\n        )\n\n    def test_dataloader(self) -> DataLoader:\n        \"\"\" Function that loads the validation set. \"\"\"\n        self._test_dataset = self.__retrieve_dataset(train=False, val=False)\n        return DataLoader(\n            dataset=self._test_dataset,\n            batch_size=self.hparam.batch_size,\n            collate_fn=self.prepare_sample,\n            num_workers=self.hparam.loader_workers,\n        )\n\n    @classmethod\n    def add_model_specific_args(\n        cls, parser: HyperOptArgumentParser\n    ) -> HyperOptArgumentParser:\n        \"\"\" Parser for Estimator specific arguments/hyperparameters. \n        :param parser: HyperOptArgumentParser obj\n        Returns:\n            - updated parser\n        \"\"\"\n        parser.add_argument(\n            \"--max_length\",\n            default=MAX_PROTEIN_STR_LEN,\n            type=int,\n            help=\"Maximum sequence length.\",\n        )\n        parser.add_argument(\n            \"--encoder_learning_rate\",\n            default=ENCODER_LR,\n            type=float,\n            help=\"Encoder specific learning rate.\",\n        )\n        parser.add_argument(\n            \"--learning_rate\",\n            default=GENERAL_LR,\n            type=float,\n            help=\"Classification head learning rate.\",\n        )\n        parser.add_argument(\n            \"--nr_frozen_epochs\",\n            default=N_FROZEN_EPOCH,\n            type=int,\n            help=\"Number of epochs we want to keep the encoder model frozen.\",\n#             tunable=True,\n            choices=[0, 1, 2, 3, 4, 5],\n        )\n        \n        # DATA Argument\n        parser.add_argument(\n            \"--train_csv\",\n            default=FILE_DIR + \"deeploc_per_protein_train.csv\",\n            type=str,\n            help=\"Path to the file containing the train data.\",\n        )\n        parser.add_argument(\n            \"--dev_csv\",\n            default=FILE_DIR + \"deeploc_per_protein_valid.csv\",\n            type=str,\n            help=\"Path to the file containing the dev data.\",\n        )\n        parser.add_argument(\n            \"--test_csv\",\n            default=FILE_DIR + \"deeploc_per_protein_test.csv\",\n            type=str,\n            help=\"Path to the file containing the test data.\",\n        )\n        \n        parser.add_argument(\n            \"--loader_workers\",\n            default=CPU_WORKERS,\n            type=int,\n            help=\"How many subprocesses to use for data loading. 0 means that \\\n                the data will be loaded in the main process.\",\n        )\n        parser.add_argument(\n            \"--gradient_checkpointing\",\n            default=GRADIENT_CHECKPOINT,\n            type=bool,\n            help=\"Enable or disable gradient checkpointing which use the cpu memory \\\n                with the gpu memory to store the model.\",\n        )\n        return parser","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:25.280429Z","iopub.execute_input":"2022-05-09T06:47:25.280939Z","iopub.status.idle":"2022-05-09T06:47:25.615709Z","shell.execute_reply.started":"2022-05-09T06:47:25.280891Z","shell.execute_reply":"2022-05-09T06:47:25.614614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# these are project-wide arguments\nparser = HyperOptArgumentParser(\n#     strategy=\"random_search\",\n    description=\"Minimalist ProtBERT Classifier\",\n    add_help=True,\n)\nparser.add_argument(\"--seed\", type=int, default=SEED, help=\"Training seed.\")\nparser.add_argument(\n    \"--save_top_k\",\n    default=N_CHECKPOINTS,\n    type=int,\n    help=\"The best k models according to the quantity monitored will be saved.\",\n)\n# Early Stopping\nparser.add_argument(\n    \"--monitor\", default=MONITOR_METRIC, type=str, help=\"Quantity to monitor.\"\n)\nparser.add_argument(\n    \"--metric_mode\",\n    default=MONITOR_MODE,\n    type=str,\n    help=\"If we want to min/max the monitored quantity.\",\n    choices=[\"auto\", \"min\", \"max\"],\n)\nparser.add_argument(\n    \"--patience\",\n    default=PATIENCE,\n    type=int,\n    help=(\n        \"Number of epochs with no improvement \"\n        \"after which training will be stopped.\"\n    ),\n)\nparser.add_argument(\n    \"--min_epochs\",\n    default=MIN_EPOCHS,\n    type=int,\n    help=\"Limits training to a minimum number of epochs\",\n)\nparser.add_argument(\n    \"--max_epochs\",\n    default=MAX_EPOCHS,\n    type=int,\n    help=\"Limits training to a max number number of epochs\",\n)\n\n# Batching\nparser.add_argument(\n    \"--batch_size\", default=BATCH_SIZE, type=int, help=\"Batch size to be used.\"\n)\nparser.add_argument(\n    \"--accumulate_grad_batches\",\n    default=ACCUM_BATCH,\n    type=int,\n    help=(\n        \"Accumulated gradients runs K small batches of size N before \"\n        \"doing a backwards pass.\"\n    ),\n)\n\n# gpu/tpu args\nparser.add_argument(\"--gpus\", type=int, default=NUM_GPU, help=\"How many gpus\")\n\n# mixed precision\nparser.add_argument(\"--precision\", type=int, default=PRECISION, help=\"full precision or mixed precision mode\")\n\n# each LightningModule defines arguments relevant to it\nparser = ProtBertBFDClassifier.add_model_specific_args(parser)\nhparam = parser.parse_known_args()[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:25.617634Z","iopub.execute_input":"2022-05-09T06:47:25.618205Z","iopub.status.idle":"2022-05-09T06:47:25.634144Z","shell.execute_reply.started":"2022-05-09T06:47:25.618164Z","shell.execute_reply":"2022-05-09T06:47:25.632869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\nhparam","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:25.636201Z","iopub.execute_input":"2022-05-09T06:47:25.636794Z","iopub.status.idle":"2022-05-09T06:47:25.896095Z","shell.execute_reply.started":"2022-05-09T06:47:25.636745Z","shell.execute_reply":"2022-05-09T06:47:25.894741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nMain training routine specific for this project\n:param hparam:\n\"\"\"\nseed_everything(hparam.seed)\n\n# ------------------------\n# 1 INIT LIGHTNING MODEL\n# ------------------------\nmodel = ProtBertBFDClassifier(hparam)\n\n# ------------------------\n# 2 INIT EARLY STOPPING\n# ------------------------\nearly_stop_callback = EarlyStopping(\n    monitor=hparam.monitor,\n    min_delta=0.0,\n    patience=hparam.patience,\n    verbose=True,\n    mode=hparam.metric_mode,\n)\n\n# --------------------------------\n# 3 INIT MODEL CHECKPOINT CALLBACK\n# -------------------------------\n# initialize Model Checkpoint Saver\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='./',\n    filename=\"{epoch}-{val_loss:.2f}-{val_acc:.2f}\",\n    save_top_k=hparam.save_top_k,\n    verbose=True,\n    monitor=hparam.monitor,\n    # period=1,\n    mode=hparam.metric_mode,\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:25.897676Z","iopub.execute_input":"2022-05-09T06:47:25.898781Z","iopub.status.idle":"2022-05-09T06:47:43.024506Z","shell.execute_reply.started":"2022-05-09T06:47:25.898735Z","shell.execute_reply":"2022-05-09T06:47:43.023338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------\n# 4 INIT TRAINER\n# ------------------------\n\nparallel_strategy = \"ddp\" if hparam.gpus > 1 else \"dp\" # ddp-spwan- tpu\n\ntrainer = Trainer(\n    gpus=hparam.gpus,\n    logger=wandb_logger,\n    strategy=parallel_strategy,\n    max_epochs=hparam.max_epochs,\n    min_epochs=hparam.min_epochs,\n    accumulate_grad_batches=hparam.accumulate_grad_batches,\n#     callbacks = [checkpoint_callback, early_stop_callback],\n    precision=hparam.precision,\n    amp_backend=AMP_BACKEND, \n#     amp_level='O2',#hparam.amp_level, # optional: for Apex backend\n    deterministic=True,\n    num_sanity_val_steps=32 # check 32 valid-batch before start\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:43.026405Z","iopub.execute_input":"2022-05-09T06:47:43.026894Z","iopub.status.idle":"2022-05-09T06:47:43.075989Z","shell.execute_reply.started":"2022-05-09T06:47:43.026791Z","shell.execute_reply":"2022-05-09T06:47:43.074744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------\n# 6 START TRAINING\n# ------------------------\n# with batch=2, its 13 min/epoch on P100\n# Remember to set MAX_EPOCHS to 10-20 epochs to maximize accuracy (needs more GPU hours on Kaggle), in this notebook we run just 5 epochs\n\nimport gc\ngc.collect()\n\ntrainer.fit(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T06:47:43.078111Z","iopub.execute_input":"2022-05-09T06:47:43.078907Z","iopub.status.idle":"2022-05-09T08:08:01.727702Z","shell.execute_reply.started":"2022-05-09T06:47:43.078836Z","shell.execute_reply":"2022-05-09T08:08:01.726679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"gc.collect()\ntrainer.test(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T08:08:01.730391Z","iopub.execute_input":"2022-05-09T08:08:01.730792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = model.load_from_checkpoint(best_checkpoint_path, hparam=hparam)\ngc.collect()\nmodel.eval()\nmodel.freeze()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = {\n  \"seq\": \"M S T D T G V S L P S Y E E D Q G S K L I R K A K E A P F V P V G I A G F A A I V A Y G L Y K L K S R G N T K M S I H L I H M R V A A Q G F V V G A M T V G M G Y S M Y R E F W A K P K P\",\n}\npredictions = model.cpu().predict(sample)\n\nprint(\"Sequence Localization Ground Truth is: {} - prediction is: {}\".format('Mitochondrion',predictions['predicted_label']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = {\n  \"seq\": \"M R C L P V F I I L L L L I P S A P S V D A Q P T T K D D V P L A S L H D N A K R A L Q M F W N K R D C C P A K L L C C N P\",\n}\n\npredictions = model.cpu().predict(sample)\n\nprint(\"Sequence Localization Ground Truth is: {} - prediction is: {}\".format('Extracellular',predictions['predicted_label']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save({\"ProtBertLoc\": model.state_dict(),\n#             \"optim\": model.optimizers.state_dict(),\n            },\n            \"./ProtBertLoc.pt\",\n        )\ncheckpoint = torch.load(\"./ProtBertLoc.pt\")\nmodel.load_state_dict(checkpoint['ProtBertLoc'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# may also use this save/load command\n# ckpt_path='./'\n# best_checkpoint_path = glob.glob(ckpt_path + \"/*\")[0]\n# print(best_checkpoint_path)\n\n# trainer.resume_from_checkpoint = best_checkpoint_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}