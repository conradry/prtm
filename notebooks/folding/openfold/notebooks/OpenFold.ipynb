{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc5-mbsX9PZC"
   },
   "source": [
    "# OpenFold Colab\n",
    "\n",
    "Runs a simplified version of [OpenFold](https://github.com/aqlaboratory/openfold) on a target sequence. Adapted from DeepMind's [official AlphaFold Colab](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb).\n",
    "\n",
    "**Differences to AlphaFold v2.0**\n",
    "\n",
    "OpenFold is a trainable PyTorch reimplementation of AlphaFold 2. For the purposes of inference, it is practically identical to the original (\"practically\" because ensembling is excluded from OpenFold (recycling is enabled, however)).\n",
    "\n",
    "In this notebook, OpenFold is run with your choice of our original OpenFold parameters or DeepMind's publicly released parameters for AlphaFold 2.\n",
    "\n",
    "**Note**\n",
    "\n",
    "Like DeepMind's official Colab, this notebook uses **no templates (homologous structures)** and a selected portion of the full [BFD database](https://bfd.mmseqs.com/).\n",
    "\n",
    "**Citing this work**\n",
    "\n",
    "Any publication that discloses findings arising from using this notebook should [cite](https://github.com/deepmind/alphafold/#citing-this-work) DeepMind's [AlphaFold paper](https://doi.org/10.1038/s41586-021-03819-2).\n",
    "\n",
    "**Licenses**\n",
    "\n",
    "This Colab supports inference with the [AlphaFold model parameters](https://github.com/deepmind/alphafold/#model-parameters-license), made available under the Creative Commons Attribution 4.0 International ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/legalcode)) license. The Colab itself is provided under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0). See the full license statement below.\n",
    "\n",
    "**More information**\n",
    "\n",
    "You can find more information about how AlphaFold/OpenFold works in DeepMind's two Nature papers:\n",
    "\n",
    "*   [AlphaFold methods paper](https://www.nature.com/articles/s41586-021-03819-2)\n",
    "*   [AlphaFold predictions of the human proteome paper](https://www.nature.com/articles/s41586-021-03828-1)\n",
    "\n",
    "FAQ on how to interpret AlphaFold/OpenFold predictions are [here](https://alphafold.ebi.ac.uk/faq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "cellView": "form",
    "id": "rowN0bVYLe9n"
   },
   "outputs": [],
   "source": [
    "sequence = 'MAAHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH'\n",
    "sequence = sequence[:32]\n",
    "\n",
    "weight_set = 'OpenFold'\n",
    "relax_prediction = True\n",
    "\n",
    "sequence = sequence.translate(str.maketrans('', '', ' \\n\\t')).upper()\n",
    "aatypes = set('ACDEFGHIKLMNPQRSTVWY')\n",
    "if not set(sequence).issubset(aatypes):\n",
    "    raise Exception(f'Input sequence contains non-amino acid letters: {set(sequence) - aatypes}. OpenFold only supports 20 standard amino acids as inputs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "woIxeCPygt7K"
   },
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "import os\n",
    "import subprocess\n",
    "import tqdm.notebook\n",
    "\n",
    "TQDM_BAR_FORMAT = '{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]'\n",
    "\n",
    "#try:\n",
    "#    with io.capture_output() as captured:\n",
    "#        !sudo apt install --quiet --yes hmmer\n",
    "\n",
    "        #%shell sudo mkdir -m 777 --parents /tmp/ramdisk\n",
    "        #%shell sudo mount -t tmpfs -o size=9G ramdisk /tmp/ramdisk\n",
    "\n",
    "        #%shell wget -q -P /content \\\n",
    "        #  https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n",
    "\n",
    "        # Install AWS CLI\n",
    "        #%shell curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "        #%shell unzip -qq awscliv2.zip\n",
    "        #%shell sudo ./aws/install\n",
    "        #%shell rm awscliv2.zip\n",
    "        #%shell rm -rf ./aws\n",
    "#except subprocess.CalledProcessError as captured:\n",
    "#    print(captured)\n",
    "#    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "VzJ5iMjTtoZw"
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "GIT_REPO='https://github.com/aqlaboratory/openfold'\n",
    "ALPHAFOLD_PARAM_SOURCE_URL = 'https://storage.googleapis.com/alphafold/alphafold_params_2022-01-19.tar'\n",
    "OPENFOLD_PARAMS_DIR = './openfold/openfold/resources/openfold_params'\n",
    "ALPHAFOLD_PARAMS_DIR = './openfold/openfold/resources/params'\n",
    "ALPHAFOLD_PARAMS_PATH = os.path.join(\n",
    "  ALPHAFOLD_PARAMS_DIR, os.path.basename(ALPHAFOLD_PARAM_SOURCE_URL)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "_FpxxMo-mvcP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conrad/miniconda3/envs/proteins/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from urllib import request\n",
    "from concurrent import futures\n",
    "import json\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import py3Dmol\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "# Prevent shell magic being broken by openmm, prevent this cryptic error:\n",
    "# \"NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\"\n",
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython import display\n",
    "#from ipywidgets import GridspecLayout\n",
    "#from ipywidgets import Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteome.models.folding.openfold.openfold import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteome.models.folding.openfold.openfold import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proteome.models.folding.openfold.openfold.data import feature_pipeline\n",
    "from proteome.models.folding.openfold.openfold.data import parsers\n",
    "from proteome.models.folding.openfold.openfold.data import data_pipeline\n",
    "from proteome.models.folding.openfold.openfold.data.tools import jackhmmer\n",
    "from proteome.models.folding.openfold.openfold.model import model\n",
    "from proteome.models.folding.openfold.openfold.np import protein\n",
    "from proteome.models.folding.openfold.openfold.np.relax import relax\n",
    "from proteome.models.folding.openfold.openfold.np.relax.utils import overwrite_b_factors\n",
    "from proteome.models.folding.openfold.openfold.utils.import_weights import import_jax_weights_\n",
    "from proteome.models.folding.openfold.openfold.utils.tensor_utils import tensor_tree_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "proteome.models.folding.openfold.openfold.model.model.AlphaFold"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = model.AlphaFold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = config.model_config(name=\"model_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "openfold_model = model.AlphaFold(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92899674"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p[1].numel() for p in openfold_model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.input_embedder.tf_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.input_embedder.msa_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.template.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_res = len(sequence)\n",
    "\n",
    "target_feat = torch.randn((1, n_res, cfg.model.input_embedder.tf_dim))\n",
    "target_feat = (target_feat == target_feat.max(dim=-1, keepdims=True).values).float()\n",
    "\n",
    "feature_dict = {\n",
    "    \"aatype\": torch.randn((1, n_res)),\n",
    "    \"target_feat\": target_feat, # one hot in rows\n",
    "    \"residue_index\": torch.arange(n_res)[None],\n",
    "    \"msa_feat\": torch.randn((1, 1, n_res, cfg.model.input_embedder.msa_dim)),\n",
    "    \"seq_mask\": torch.ones((1, n_res)),\n",
    "    \"msa_mask\": torch.ones((1, 2, n_res)),\n",
    "    \"pair_mask\": torch.ones((1, 32, n_res)),\n",
    "    \"extra_msa_mask\": torch.ones((1, 0, n_res)),\n",
    "    \n",
    "    \"template_mask\": torch.randn((1, n_res)),\n",
    "    \"template_aatype\": torch.randn((1, n_res)),\n",
    "    \"template_all_atom_positions\": torch.randn((1, n_res)),\n",
    "    \"template_all_atom_mask\": torch.randn((1, n_res)),\n",
    "    \"template_pseudo_beta\": torch.randn((1, n_res)),\n",
    "    \"template_pseudo_beta_mask\": torch.randn((1, n_res)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "feature_dict.update(data_pipeline.make_sequence_features(sequence, 'test', n_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict.update(data_pipeline.make_msa_features([[sequence]], deletion_matrices=[n_res * [n_res * [0]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_templates = 1\n",
    "def _placeholder_template_feats(num_templates_, num_res_):\n",
    "    return {\n",
    "      'template_aatype': torch.zeros((num_templates_, num_res_, 22), dtype=torch.int64),\n",
    "      'template_all_atom_positions': torch.zeros((num_templates_, num_res_, 37, 3), dtype=torch.float32),\n",
    "      'template_all_atom_mask': torch.zeros((num_templates_, num_res_, 37), dtype=torch.float32),\n",
    "      'template_domain_names': torch.zeros((num_templates_,), dtype=torch.float32),\n",
    "      'template_sum_probs': torch.zeros((num_templates_, 1), dtype=torch.float32),\n",
    "    }\n",
    "\n",
    "feature_dict.update(_placeholder_template_feats(num_templates, n_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aatype (32, 21)\n",
      "between_segment_residues (32,)\n",
      "domain_name (1,)\n",
      "residue_index (32,)\n",
      "seq_length (32,)\n",
      "sequence (1,)\n",
      "deletion_matrix_int (1, 32)\n",
      "msa (1, 32)\n",
      "num_alignments (32,)\n",
      "template_aatype torch.Size([1, 32, 22])\n",
      "template_all_atom_positions torch.Size([1, 32, 37, 3])\n",
      "template_all_atom_mask torch.Size([1, 32, 37])\n",
      "template_domain_names torch.Size([1])\n",
      "template_sum_probs torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for k,v in feature_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dict[\"num_alignments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_length torch.Size([32])\n",
      "num_alignments torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "pipeline = feature_pipeline.FeaturePipeline(cfg.data)\n",
    "processed_feature_dict = pipeline.process_features(\n",
    "  feature_dict, mode='predict'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_feature_dict = tensor_tree_map(\n",
    "    lambda t: t.cpu(), processed_feature_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 22, 4])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_feature_dict[\"target_feat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_feature_dict[\"residue_index\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 32, 49, 4])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_feature_dict[\"msa_feat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target feat shape torch.Size([32, 22])\n",
      "Linear weight torch.Size([128, 22])\n",
      "tf shape torch.Size([32, 22])\n",
      "target feat shape torch.Size([32, 22])\n",
      "Linear weight torch.Size([128, 22])\n",
      "tf shape torch.Size([32, 22])\n",
      "target feat shape torch.Size([32, 22])\n",
      "Linear weight torch.Size([128, 22])\n",
      "tf shape torch.Size([32, 22])\n",
      "target feat shape torch.Size([32, 22])\n",
      "Linear weight torch.Size([128, 22])\n",
      "tf shape torch.Size([32, 22])\n"
     ]
    }
   ],
   "source": [
    "openfold_model.eval()\n",
    "with torch.no_grad():\n",
    "    res = openfold_model(processed_feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['atom_positions', 'aatype', 'atom_mask', 'residue_index', 'b_factors', 'chain_index', 'remark', 'parents', 'parents_chain_index'])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(unrelaxed_protein).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_plddt = res['plddt'].mean()\n",
    "final_atom_mask = res['final_atom_mask']\n",
    "b_factors = res['plddt'][:, None] * final_atom_mask\n",
    "unrelaxed_protein = protein.from_prediction(\n",
    "  processed_feature_dict, res, b_factors=b_factors\n",
    ")\n",
    "unrelaxed_protein = protein.Protein(**{k: getattr(unrelaxed_protein, k).numpy() for k in ['atom_positions', 'aatype', 'atom_mask', 'residue_index', 'b_factors']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_pdb \u001b[38;5;241m=\u001b[39m \u001b[43mprotein\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pdb\u001b[49m\u001b[43m(\u001b[49m\u001b[43munrelaxed_protein\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/proteome/proteome/models/folding/openfold/openfold/np/protein.py:332\u001b[0m, in \u001b[0;36mto_pdb\u001b[0;34m(prot)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Add all atom sites.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m--> 332\u001b[0m     res_name_3 \u001b[38;5;241m=\u001b[39m \u001b[43mres_1to3\u001b[49m\u001b[43m(\u001b[49m\u001b[43maatype\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m atom_name, pos, mask, b_factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    334\u001b[0m         atom_types, atom_positions[i], atom_mask[i], b_factors[i]\n\u001b[1;32m    335\u001b[0m     ):\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n",
      "File \u001b[0;32m~/repos/proteome/proteome/models/folding/openfold/openfold/np/protein.py:307\u001b[0m, in \u001b[0;36mto_pdb.<locals>.<lambda>\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts a `Protein` instance to a PDB string.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m  PDB string.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    306\u001b[0m restypes \u001b[38;5;241m=\u001b[39m residue_constants\u001b[38;5;241m.\u001b[39mrestypes \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 307\u001b[0m res_1to3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m r: residue_constants\u001b[38;5;241m.\u001b[39mrestype_1to3\u001b[38;5;241m.\u001b[39mget(\u001b[43mrestypes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    308\u001b[0m atom_types \u001b[38;5;241m=\u001b[39m residue_constants\u001b[38;5;241m.\u001b[39matom_types\n\u001b[1;32m    310\u001b[0m pdb_lines \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "best_pdb = protein.to_pdb(unrelaxed_protein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    if 'predicted_aligned_error' in prediction_result:\n",
    "      pae_outputs[model_name] = (\n",
    "          prediction_result['predicted_aligned_error'],\n",
    "          prediction_result['max_predicted_aligned_error']\n",
    "      )\n",
    "    else:\n",
    "      # Get the pLDDT confidence metrics. Do not put pTM models here as they\n",
    "      # should never get selected.\n",
    "      plddts[model_name] = prediction_result['plddt']\n",
    "\n",
    "    # Set the b-factors to the per-residue plddt.\n",
    "    final_atom_mask = prediction_result['final_atom_mask']\n",
    "    b_factors = prediction_result['plddt'][:, None] * final_atom_mask\n",
    "    unrelaxed_protein = protein.from_prediction(\n",
    "      processed_feature_dict, prediction_result, b_factors=b_factors\n",
    "    )\n",
    "    unrelaxed_proteins[model_name] = unrelaxed_protein\n",
    "\n",
    "    # Delete unused outputs to save memory.\n",
    "    del openfold_model\n",
    "    del processed_feature_dict\n",
    "    del prediction_result\n",
    "    pbar.update(n=1)\n",
    "\n",
    "  # Find the best model according to the mean pLDDT.\n",
    "  best_model_name = max(plddts.keys(), key=lambda x: plddts[x].mean())\n",
    "  best_pdb = protein.to_pdb(unrelaxed_proteins[best_model_name])\n",
    "\n",
    "  # --- AMBER relax the best model ---\n",
    "  if(relax_prediction):\n",
    "    pbar.set_description(f'AMBER relaxation')\n",
    "    amber_relaxer = relax.AmberRelaxation(\n",
    "        max_iterations=0,\n",
    "        tolerance=2.39,\n",
    "        stiffness=10.0,\n",
    "        exclude_residues=[],\n",
    "        max_outer_iterations=20,\n",
    "        use_gpu=False,\n",
    "    )\n",
    "    relaxed_pdb, _, _ = amber_relaxer.process(\n",
    "        prot=unrelaxed_proteins[best_model_name]\n",
    "    )\n",
    "    best_pdb = relaxed_pdb\n",
    "\n",
    "  # Write out the prediction\n",
    "  pred_output_path = os.path.join(output_dir, 'selected_prediction.pdb')\n",
    "  with open(pred_output_path, 'w') as f:\n",
    "    f.write(best_pdb)\n",
    "\n",
    "  pbar.update(n=1)  # Finished AMBER relax.\n",
    "\n",
    "# Construct multiclass b-factors to indicate confidence bands\n",
    "# 0=very low, 1=low, 2=confident, 3=very high\n",
    "banded_b_factors = []\n",
    "for plddt in plddts[best_model_name]:\n",
    "  for idx, (min_val, max_val, _) in enumerate(PLDDT_BANDS):\n",
    "    if plddt >= min_val and plddt <= max_val:\n",
    "      banded_b_factors.append(idx)\n",
    "      break\n",
    "banded_b_factors = np.array(banded_b_factors)[:, None] * final_atom_mask\n",
    "to_visualize_pdb = overwrite_b_factors(best_pdb, banded_b_factors)\n",
    "\n",
    "# --- Visualise the prediction & confidence ---\n",
    "show_sidechains = True\n",
    "def plot_plddt_legend():\n",
    "  \"\"\"Plots the legend for pLDDT.\"\"\"\n",
    "  thresh = [\n",
    "            'Very low (pLDDT < 50)',\n",
    "            'Low (70 > pLDDT > 50)',\n",
    "            'Confident (90 > pLDDT > 70)',\n",
    "            'Very high (pLDDT > 90)']\n",
    "\n",
    "  colors = [x[2] for x in PLDDT_BANDS]\n",
    "\n",
    "  plt.figure(figsize=(2, 2))\n",
    "  for c in colors:\n",
    "    plt.bar(0, 0, color=c)\n",
    "  plt.legend(thresh, frameon=False, loc='center', fontsize=20)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  ax = plt.gca()\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  ax.spines['top'].set_visible(False)\n",
    "  ax.spines['left'].set_visible(False)\n",
    "  ax.spines['bottom'].set_visible(False)\n",
    "  plt.title('Model Confidence', fontsize=20, pad=20)\n",
    "  return plt\n",
    "\n",
    "# Color the structure by per-residue pLDDT\n",
    "color_map = {i: bands[2] for i, bands in enumerate(PLDDT_BANDS)}\n",
    "view = py3Dmol.view(width=800, height=600)\n",
    "view.addModelsAsFrames(to_visualize_pdb)\n",
    "style = {'cartoon': {\n",
    "    'colorscheme': {\n",
    "        'prop': 'b',\n",
    "        'map': color_map}\n",
    "        }}\n",
    "if show_sidechains:\n",
    "  style['stick'] = {}\n",
    "view.setStyle({'model': -1}, style)\n",
    "view.zoomTo()\n",
    "\n",
    "grid = GridspecLayout(1, 2)\n",
    "out = Output()\n",
    "with out:\n",
    "  view.show()\n",
    "grid[0, 0] = out\n",
    "\n",
    "out = Output()\n",
    "with out:\n",
    "  plot_plddt_legend().show()\n",
    "grid[0, 1] = out\n",
    "\n",
    "display.display(grid)\n",
    "\n",
    "# Display pLDDT and predicted aligned error (if output by the model).\n",
    "if pae_outputs:\n",
    "  num_plots = 2\n",
    "else:\n",
    "  num_plots = 1\n",
    "\n",
    "plt.figure(figsize=[8 * num_plots, 6])\n",
    "plt.subplot(1, num_plots, 1)\n",
    "plt.plot(plddts[best_model_name])\n",
    "plt.title('Predicted LDDT')\n",
    "plt.xlabel('Residue')\n",
    "plt.ylabel('pLDDT')\n",
    "\n",
    "if num_plots == 2:\n",
    "  plt.subplot(1, 2, 2)\n",
    "  pae, max_pae = list(pae_outputs.values())[0]\n",
    "  plt.imshow(pae, vmin=0., vmax=max_pae, cmap='Greens_r')\n",
    "  plt.colorbar(fraction=0.046, pad=0.04)\n",
    "  plt.title('Predicted Aligned Error')\n",
    "  plt.xlabel('Scored residue')\n",
    "  plt.ylabel('Aligned residue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 37, 3])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['final_atom_positions'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4JpOs6oA-QS"
   },
   "source": [
    "## Making a prediction\n",
    "\n",
    "Note that the search against databases and the actual prediction can take some time, from minutes to hours, depending on the length of the protein and what type of GPU you are allocated by Colab (see FAQ below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2tTeTTsLKPjB"
   },
   "outputs": [],
   "source": [
    "#@title Search against genetic databases\n",
    "\n",
    "#@markdown Once this cell has been executed, you will see\n",
    "#@markdown statistics about the multiple sequence alignment \n",
    "#@markdown (MSA) that will be used by OpenFold. In particular, \n",
    "#@markdown you’ll see how well each residue is covered by similar \n",
    "#@markdown sequences in the MSA.\n",
    "\n",
    "# --- Find the closest source ---\n",
    "test_url_pattern = 'https://storage.googleapis.com/alphafold-colab{:s}/latest/uniref90_2021_03.fasta.1'\n",
    "ex = futures.ThreadPoolExecutor(3)\n",
    "def fetch(source):\n",
    "  request.urlretrieve(test_url_pattern.format(source))\n",
    "  return source\n",
    "fs = [ex.submit(fetch, source) for source in ['', '-europe', '-asia']]\n",
    "source = None\n",
    "for f in futures.as_completed(fs):\n",
    "  source = f.result()\n",
    "  ex.shutdown()\n",
    "  break\n",
    "\n",
    "# --- Search against genetic databases ---\n",
    "with open('target.fasta', 'wt') as f:\n",
    "  f.write(f'>query\\n{sequence}')\n",
    "\n",
    "# Run the search against chunks of genetic databases (since the genetic\n",
    "# databases don't fit in Colab ramdisk).\n",
    "\n",
    "jackhmmer_binary_path = '/usr/bin/jackhmmer'\n",
    "dbs = []\n",
    "\n",
    "num_jackhmmer_chunks = {'uniref90': 59, 'smallbfd': 17, 'mgnify': 71}\n",
    "total_jackhmmer_chunks = sum(num_jackhmmer_chunks.values())\n",
    "with tqdm.notebook.tqdm(total=total_jackhmmer_chunks, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
    "  def jackhmmer_chunk_callback(i):\n",
    "    pbar.update(n=1)\n",
    "\n",
    "  pbar.set_description('Searching uniref90')\n",
    "  jackhmmer_uniref90_runner = jackhmmer.Jackhmmer(\n",
    "      binary_path=jackhmmer_binary_path,\n",
    "      database_path=f'https://storage.googleapis.com/alphafold-colab{source}/latest/uniref90_2021_03.fasta',\n",
    "      get_tblout=True,\n",
    "      num_streamed_chunks=num_jackhmmer_chunks['uniref90'],\n",
    "      streaming_callback=jackhmmer_chunk_callback,\n",
    "      z_value=135301051)\n",
    "  dbs.append(('uniref90', jackhmmer_uniref90_runner.query('target.fasta')))\n",
    "\n",
    "  pbar.set_description('Searching smallbfd')\n",
    "  jackhmmer_smallbfd_runner = jackhmmer.Jackhmmer(\n",
    "      binary_path=jackhmmer_binary_path,\n",
    "      database_path=f'https://storage.googleapis.com/alphafold-colab{source}/latest/bfd-first_non_consensus_sequences.fasta',\n",
    "      get_tblout=True,\n",
    "      num_streamed_chunks=num_jackhmmer_chunks['smallbfd'],\n",
    "      streaming_callback=jackhmmer_chunk_callback,\n",
    "      z_value=65984053)\n",
    "  dbs.append(('smallbfd', jackhmmer_smallbfd_runner.query('target.fasta')))\n",
    "\n",
    "  pbar.set_description('Searching mgnify')\n",
    "  jackhmmer_mgnify_runner = jackhmmer.Jackhmmer(\n",
    "      binary_path=jackhmmer_binary_path,\n",
    "      database_path=f'https://storage.googleapis.com/alphafold-colab{source}/latest/mgy_clusters_2019_05.fasta',\n",
    "      get_tblout=True,\n",
    "      num_streamed_chunks=num_jackhmmer_chunks['mgnify'],\n",
    "      streaming_callback=jackhmmer_chunk_callback,\n",
    "      z_value=304820129)\n",
    "  dbs.append(('mgnify', jackhmmer_mgnify_runner.query('target.fasta')))\n",
    "\n",
    "\n",
    "# --- Extract the MSAs and visualize ---\n",
    "# Extract the MSAs from the Stockholm files.\n",
    "# NB: deduplication happens later in data_pipeline.make_msa_features.\n",
    "\n",
    "mgnify_max_hits = 501\n",
    "\n",
    "msas = []\n",
    "deletion_matrices = []\n",
    "full_msa = []\n",
    "for db_name, db_results in dbs:\n",
    "  unsorted_results = []\n",
    "  for i, result in enumerate(db_results):\n",
    "    msa, deletion_matrix, target_names = parsers.parse_stockholm(result['sto'])\n",
    "    e_values_dict = parsers.parse_e_values_from_tblout(result['tbl'])\n",
    "    e_values = [e_values_dict[t.split('/')[0]] for t in target_names]\n",
    "    zipped_results = zip(msa, deletion_matrix, target_names, e_values)\n",
    "    if i != 0:\n",
    "      # Only take query from the first chunk\n",
    "      zipped_results = [x for x in zipped_results if x[2] != 'query']\n",
    "    unsorted_results.extend(zipped_results)\n",
    "  sorted_by_evalue = sorted(unsorted_results, key=lambda x: x[3])\n",
    "  db_msas, db_deletion_matrices, _, _ = zip(*sorted_by_evalue)\n",
    "  if db_msas:\n",
    "    if db_name == 'mgnify':\n",
    "      db_msas = db_msas[:mgnify_max_hits]\n",
    "      db_deletion_matrices = db_deletion_matrices[:mgnify_max_hits]\n",
    "    full_msa.extend(db_msas)\n",
    "    msas.append(db_msas)\n",
    "    deletion_matrices.append(db_deletion_matrices)\n",
    "    msa_size = len(set(db_msas))\n",
    "    print(f'{msa_size} Sequences Found in {db_name}')\n",
    "\n",
    "deduped_full_msa = list(dict.fromkeys(full_msa))\n",
    "total_msa_size = len(deduped_full_msa)\n",
    "print(f'\\n{total_msa_size} Sequences Found in Total\\n')\n",
    "\n",
    "aa_map = {restype: i for i, restype in enumerate('ABCDEFGHIJKLMNOPQRSTUVWXYZ-')}\n",
    "msa_arr = np.array([[aa_map[aa] for aa in seq] for seq in deduped_full_msa])\n",
    "num_alignments, num_res = msa_arr.shape\n",
    "\n",
    "fig = plt.figure(figsize=(12, 3))\n",
    "plt.title('Per-Residue Count of Non-Gap Amino Acids in the MSA')\n",
    "plt.plot(np.sum(msa_arr != aa_map['-'], axis=0), color='black')\n",
    "plt.ylabel('Non-Gap Count')\n",
    "plt.yticks(range(0, num_alignments + 1, max(1, int(num_alignments / 3))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XUo6foMQxwS2"
   },
   "outputs": [],
   "source": [
    "#@title Run OpenFold and download prediction\n",
    "\n",
    "#@markdown Once this cell has been executed, a zip-archive with \n",
    "#@markdown the obtained prediction will be automatically downloaded \n",
    "#@markdown to your computer.\n",
    "\n",
    "# Color bands for visualizing plddt\n",
    "PLDDT_BANDS = [\n",
    "  (0, 50, '#FF7D45'),\n",
    "  (50, 70, '#FFDB13'),\n",
    "  (70, 90, '#65CBF3'),\n",
    "  (90, 100, '#0053D6')\n",
    "]\n",
    "\n",
    "# --- Run the model ---\n",
    "model_names = [ \n",
    "  'finetuning_3.pt', \n",
    "  'finetuning_4.pt', \n",
    "  'finetuning_5.pt', \n",
    "  'finetuning_ptm_2.pt',\n",
    "  'finetuning_no_templ_ptm_1.pt'\n",
    "]\n",
    "\n",
    "def _placeholder_template_feats(num_templates_, num_res_):\n",
    "  return {\n",
    "      'template_aatype': np.zeros((num_templates_, num_res_, 22), dtype=np.int64),\n",
    "      'template_all_atom_positions': np.zeros((num_templates_, num_res_, 37, 3), dtype=np.float32),\n",
    "      'template_all_atom_mask': np.zeros((num_templates_, num_res_, 37), dtype=np.float32),\n",
    "      'template_domain_names': np.zeros((num_templates_,), dtype=np.float32),\n",
    "      'template_sum_probs': np.zeros((num_templates_, 1), dtype=np.float32),\n",
    "  }\n",
    "\n",
    "output_dir = 'prediction'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "plddts = {}\n",
    "pae_outputs = {}\n",
    "unrelaxed_proteins = {}\n",
    "\n",
    "with tqdm.notebook.tqdm(total=len(model_names) + 1, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
    "  for i, model_name in list(enumerate(model_names)):\n",
    "    pbar.set_description(f'Running {model_name}')\n",
    "    num_templates = 1 # dummy number --- is ignored\n",
    "    num_res = len(sequence)\n",
    "    \n",
    "    feature_dict = {}\n",
    "    feature_dict.update(data_pipeline.make_sequence_features(sequence, 'test', num_res))\n",
    "    feature_dict.update(data_pipeline.make_msa_features(msas, deletion_matrices=deletion_matrices))\n",
    "    feature_dict.update(_placeholder_template_feats(num_templates, num_res))\n",
    "\n",
    "    if(weight_set == \"AlphaFold\"):\n",
    "      config_preset = f\"model_{i}\"\n",
    "    else:\n",
    "      if(\"_no_templ_\" in model_name):\n",
    "        config_preset = \"model_3\"\n",
    "      else:\n",
    "        config_preset = \"model_1\"\n",
    "      if(\"_ptm_\" in model_name):\n",
    "        config_preset += \"_ptm\"\n",
    "\n",
    "    cfg = config.model_config(config_preset)\n",
    "    openfold_model = model.AlphaFold(cfg)\n",
    "    openfold_model = openfold_model.eval()\n",
    "    if(weight_set == \"AlphaFold\"):\n",
    "      params_name = os.path.join(\n",
    "        ALPHAFOLD_PARAMS_DIR, f\"params_{config_preset}.npz\"\n",
    "      )\n",
    "      import_jax_weights_(openfold_model, params_name, version=config_preset)\n",
    "    elif(weight_set == \"OpenFold\"):\n",
    "      params_name = os.path.join(\n",
    "        OPENFOLD_PARAMS_DIR,\n",
    "        model_name,\n",
    "      )\n",
    "      d = torch.load(params_name)\n",
    "      openfold_model.load_state_dict(d)\n",
    "    else:\n",
    "      raise ValueError(f\"Invalid weight set: {weight_set}\")\n",
    "\n",
    "    openfold_model = openfold_model.cuda()\n",
    "\n",
    "    pipeline = feature_pipeline.FeaturePipeline(cfg.data)\n",
    "    processed_feature_dict = pipeline.process_features(\n",
    "      feature_dict, mode='predict'\n",
    "    )\n",
    "\n",
    "    processed_feature_dict = tensor_tree_map(\n",
    "        lambda t: t.cuda(), processed_feature_dict\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "      prediction_result = openfold_model(processed_feature_dict)\n",
    "\n",
    "    # Move the batch and output to np for further processing\n",
    "    processed_feature_dict = tensor_tree_map(\n",
    "      lambda t: np.array(t[..., -1].cpu()), processed_feature_dict\n",
    "    )\n",
    "    prediction_result = tensor_tree_map(\n",
    "      lambda t: np.array(t.cpu()), prediction_result\n",
    "    )\n",
    "\n",
    "    mean_plddt = prediction_result['plddt'].mean()\n",
    "\n",
    "    if 'predicted_aligned_error' in prediction_result:\n",
    "      pae_outputs[model_name] = (\n",
    "          prediction_result['predicted_aligned_error'],\n",
    "          prediction_result['max_predicted_aligned_error']\n",
    "      )\n",
    "    else:\n",
    "      # Get the pLDDT confidence metrics. Do not put pTM models here as they\n",
    "      # should never get selected.\n",
    "      plddts[model_name] = prediction_result['plddt']\n",
    "\n",
    "    # Set the b-factors to the per-residue plddt.\n",
    "    final_atom_mask = prediction_result['final_atom_mask']\n",
    "    b_factors = prediction_result['plddt'][:, None] * final_atom_mask\n",
    "    unrelaxed_protein = protein.from_prediction(\n",
    "      processed_feature_dict, prediction_result, b_factors=b_factors\n",
    "    )\n",
    "    unrelaxed_proteins[model_name] = unrelaxed_protein\n",
    "\n",
    "    # Delete unused outputs to save memory.\n",
    "    del openfold_model\n",
    "    del processed_feature_dict\n",
    "    del prediction_result\n",
    "    pbar.update(n=1)\n",
    "\n",
    "  # Find the best model according to the mean pLDDT.\n",
    "  best_model_name = max(plddts.keys(), key=lambda x: plddts[x].mean())\n",
    "  best_pdb = protein.to_pdb(unrelaxed_proteins[best_model_name])\n",
    "\n",
    "  # --- AMBER relax the best model ---\n",
    "  if(relax_prediction):\n",
    "    pbar.set_description(f'AMBER relaxation')\n",
    "    amber_relaxer = relax.AmberRelaxation(\n",
    "        max_iterations=0,\n",
    "        tolerance=2.39,\n",
    "        stiffness=10.0,\n",
    "        exclude_residues=[],\n",
    "        max_outer_iterations=20,\n",
    "        use_gpu=False,\n",
    "    )\n",
    "    relaxed_pdb, _, _ = amber_relaxer.process(\n",
    "        prot=unrelaxed_proteins[best_model_name]\n",
    "    )\n",
    "    best_pdb = relaxed_pdb\n",
    "\n",
    "  # Write out the prediction\n",
    "  pred_output_path = os.path.join(output_dir, 'selected_prediction.pdb')\n",
    "  with open(pred_output_path, 'w') as f:\n",
    "    f.write(best_pdb)\n",
    "\n",
    "  pbar.update(n=1)  # Finished AMBER relax.\n",
    "\n",
    "# Construct multiclass b-factors to indicate confidence bands\n",
    "# 0=very low, 1=low, 2=confident, 3=very high\n",
    "banded_b_factors = []\n",
    "for plddt in plddts[best_model_name]:\n",
    "  for idx, (min_val, max_val, _) in enumerate(PLDDT_BANDS):\n",
    "    if plddt >= min_val and plddt <= max_val:\n",
    "      banded_b_factors.append(idx)\n",
    "      break\n",
    "banded_b_factors = np.array(banded_b_factors)[:, None] * final_atom_mask\n",
    "to_visualize_pdb = overwrite_b_factors(best_pdb, banded_b_factors)\n",
    "\n",
    "# --- Visualise the prediction & confidence ---\n",
    "show_sidechains = True\n",
    "def plot_plddt_legend():\n",
    "  \"\"\"Plots the legend for pLDDT.\"\"\"\n",
    "  thresh = [\n",
    "            'Very low (pLDDT < 50)',\n",
    "            'Low (70 > pLDDT > 50)',\n",
    "            'Confident (90 > pLDDT > 70)',\n",
    "            'Very high (pLDDT > 90)']\n",
    "\n",
    "  colors = [x[2] for x in PLDDT_BANDS]\n",
    "\n",
    "  plt.figure(figsize=(2, 2))\n",
    "  for c in colors:\n",
    "    plt.bar(0, 0, color=c)\n",
    "  plt.legend(thresh, frameon=False, loc='center', fontsize=20)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  ax = plt.gca()\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  ax.spines['top'].set_visible(False)\n",
    "  ax.spines['left'].set_visible(False)\n",
    "  ax.spines['bottom'].set_visible(False)\n",
    "  plt.title('Model Confidence', fontsize=20, pad=20)\n",
    "  return plt\n",
    "\n",
    "# Color the structure by per-residue pLDDT\n",
    "color_map = {i: bands[2] for i, bands in enumerate(PLDDT_BANDS)}\n",
    "view = py3Dmol.view(width=800, height=600)\n",
    "view.addModelsAsFrames(to_visualize_pdb)\n",
    "style = {'cartoon': {\n",
    "    'colorscheme': {\n",
    "        'prop': 'b',\n",
    "        'map': color_map}\n",
    "        }}\n",
    "if show_sidechains:\n",
    "  style['stick'] = {}\n",
    "view.setStyle({'model': -1}, style)\n",
    "view.zoomTo()\n",
    "\n",
    "grid = GridspecLayout(1, 2)\n",
    "out = Output()\n",
    "with out:\n",
    "  view.show()\n",
    "grid[0, 0] = out\n",
    "\n",
    "out = Output()\n",
    "with out:\n",
    "  plot_plddt_legend().show()\n",
    "grid[0, 1] = out\n",
    "\n",
    "display.display(grid)\n",
    "\n",
    "# Display pLDDT and predicted aligned error (if output by the model).\n",
    "if pae_outputs:\n",
    "  num_plots = 2\n",
    "else:\n",
    "  num_plots = 1\n",
    "\n",
    "plt.figure(figsize=[8 * num_plots, 6])\n",
    "plt.subplot(1, num_plots, 1)\n",
    "plt.plot(plddts[best_model_name])\n",
    "plt.title('Predicted LDDT')\n",
    "plt.xlabel('Residue')\n",
    "plt.ylabel('pLDDT')\n",
    "\n",
    "if num_plots == 2:\n",
    "  plt.subplot(1, 2, 2)\n",
    "  pae, max_pae = list(pae_outputs.values())[0]\n",
    "  plt.imshow(pae, vmin=0., vmax=max_pae, cmap='Greens_r')\n",
    "  plt.colorbar(fraction=0.046, pad=0.04)\n",
    "  plt.title('Predicted Aligned Error')\n",
    "  plt.xlabel('Scored residue')\n",
    "  plt.ylabel('Aligned residue')\n",
    "\n",
    "# Save pLDDT and predicted aligned error (if it exists)\n",
    "pae_output_path = os.path.join(output_dir, 'predicted_aligned_error.json')\n",
    "if pae_outputs:\n",
    "  # Save predicted aligned error in the same format as the AF EMBL DB\n",
    "  rounded_errors = np.round(pae.astype(np.float64), decimals=1)\n",
    "  indices = np.indices((len(rounded_errors), len(rounded_errors))) + 1\n",
    "  indices_1 = indices[0].flatten().tolist()\n",
    "  indices_2 = indices[1].flatten().tolist()\n",
    "  pae_data = json.dumps([{\n",
    "      'residue1': indices_1,\n",
    "      'residue2': indices_2,\n",
    "      'distance': rounded_errors.flatten().tolist(),\n",
    "      'max_predicted_aligned_error': max_pae.item()\n",
    "  }],\n",
    "                        indent=None,\n",
    "                        separators=(',', ':'))\n",
    "  with open(pae_output_path, 'w') as f:\n",
    "    f.write(pae_data)\n",
    "\n",
    "\n",
    "# --- Download the predictions ---\n",
    "shutil.make_archive(base_name='prediction', format='zip', root_dir=output_dir)\n",
    "files.download(f'{output_dir}.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUQAn5LYC5n4"
   },
   "source": [
    "### Interpreting the prediction\n",
    "\n",
    "Please see the [AlphaFold methods paper](https://www.nature.com/articles/s41586-021-03819-2) and the [AlphaFold predictions of the human proteome paper](https://www.nature.com/articles/s41586-021-03828-1), as well as [DeepMind's FAQ](https://alphafold.ebi.ac.uk/faq) on how to interpret AlphaFold/OpenFold predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeb2z8DIA4om"
   },
   "source": [
    "## FAQ & Troubleshooting\n",
    "\n",
    "\n",
    "*   How do I get a predicted protein structure for my protein?\n",
    "    *   Click on the _Connect_ button on the top right to get started.\n",
    "    *   Paste the amino acid sequence of your protein (without any headers) into the “Enter the amino acid sequence to fold”.\n",
    "    *   Run all cells in the Colab, either by running them individually (with the play button on the left side) or via _Runtime_ > _Run all._\n",
    "    *   The predicted protein structure will be downloaded once all cells have been executed. Note: This can take minutes to hours - see below.\n",
    "*   How long will this take?\n",
    "    *   Downloading the OpenFold source code can take up to a few minutes.\n",
    "    *   Downloading and installing the third-party software can take up to a few minutes.\n",
    "    *   The search against genetic databases can take minutes to hours.\n",
    "    *   Running OpenFold and generating the prediction can take minutes to hours, depending on the length of your protein and on which GPU-type Colab has assigned you.\n",
    "*   My Colab no longer seems to be doing anything, what should I do?\n",
    "    *   Some steps may take minutes to hours to complete.\n",
    "    *   Sometimes, running the \"installation\" cells more than once can corrupt the OpenFold installation.\n",
    "    *   If nothing happens or if you receive an error message, try restarting your Colab runtime via _Runtime_ > _Restart runtime_.\n",
    "    *   If this doesn’t help, reset your Colab runtime via _Runtime_ > _Factory reset runtime_.\n",
    "*   How does what's run in this notebook compare to the full versions of Alphafold/Openfold?\n",
    "    *   This Colab version of OpenFold searches a selected portion of the BFD dataset and currently doesn’t use templates, so its accuracy is reduced in comparison to the full version, which is analogous to what's described in the [AlphaFold paper](https://doi.org/10.1038/s41586-021-03819-2) and [Github repo](https://github.com/deepmind/alphafold/). The full version of OpenFold can be run from our own [GitHub repo](https://github.com/aqlaboratory/openfold).\n",
    "*   What is a Colab?\n",
    "    *   See the [Colab FAQ](https://research.google.com/colaboratory/faq.html).\n",
    "*   I received a warning “Notebook requires high RAM”, what do I do?\n",
    "    *   The resources allocated to your Colab vary. See the [Colab FAQ](https://research.google.com/colaboratory/faq.html) for more details.\n",
    "    *   You can execute the Colab nonetheless.\n",
    "*   I received an error “Colab CPU runtime not supported” or “No GPU/TPU found”, what do I do?\n",
    "    *   Colab CPU runtime is not supported. Try changing your runtime via _Runtime_ > _Change runtime type_ > _Hardware accelerator_ > _GPU_.\n",
    "    *   The type of GPU allocated to your Colab varies. See the [Colab FAQ](https://research.google.com/colaboratory/faq.html) for more details.\n",
    "    *   If you receive “Cannot connect to GPU backend”, you can try again later to see if Colab allocates you a GPU.\n",
    "    *   [Colab Pro](https://colab.research.google.com/signup) offers priority access to GPUs. \n",
    "*   Does this tool install anything on my computer?\n",
    "    *   No, everything happens in the cloud on Google Colab.\n",
    "    *   At the end of the Colab execution a zip-archive with the obtained prediction will be automatically downloaded to your computer.\n",
    "*   How should I share feedback and bug reports?\n",
    "    *   Please share any feedback and bug reports as an [issue](https://github.com/aqlaboratory/openfold/issues) on Github.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfPhvYgKC81B"
   },
   "source": [
    "# License and Disclaimer\n",
    "\n",
    "This Colab notebook and other information provided is for theoretical modelling only, caution should be exercised in its use. It is provided ‘as-is’ without any warranty of any kind, whether expressed or implied. Information is not intended to be a substitute for professional medical advice, diagnosis, or treatment, and does not constitute medical or other professional advice.\n",
    "\n",
    "## AlphaFold/OpenFold Code License\n",
    "\n",
    "Copyright 2021 AlQuraishi Laboratory\n",
    "\n",
    "Copyright 2021 DeepMind Technologies Limited.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0.\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "## Model Parameters License\n",
    "\n",
    "DeepMind's AlphaFold parameters are made available under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license. You can find details at: https://creativecommons.org/licenses/by/4.0/legalcode\n",
    "\n",
    "\n",
    "## Third-party software\n",
    "\n",
    "Use of the third-party software, libraries or code referred to in this notebook may be governed by separate terms and conditions or license provisions. Your use of the third-party software, libraries or code is subject to any such terms and you should check that you can comply with any applicable restrictions or terms and conditions before use.\n",
    "\n",
    "\n",
    "## Mirrored Databases\n",
    "\n",
    "The following databases have been mirrored by DeepMind, and are available with reference to the following:\n",
    "* UniRef90: v2021\\_03 (unmodified), by The UniProt Consortium, available under a [Creative Commons Attribution-NoDerivatives 4.0 International License](http://creativecommons.org/licenses/by-nd/4.0/).\n",
    "* MGnify: v2019\\_05 (unmodified), by Mitchell AL et al., available free of all copyright restrictions and made fully and freely available for both non-commercial and commercial use under [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).\n",
    "* BFD: (modified), by Steinegger M. and Söding J., modified by DeepMind, available under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by/4.0/). See the Methods section of the [AlphaFold proteome paper](https://www.nature.com/articles/s41586-021-03828-1) for details."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OpenFold.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
