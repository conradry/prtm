--- /content/alphafold_copy/alphafold/model/modules.py	2022-06-16 23:22:42.489230552 +0000
+++ /content/alphafold/alphafold/model/modules.py	2022-06-16 23:33:50.394462860 +0000
@@ -322,7 +322,7 @@
                 recycle_idx,
                 compute_loss=compute_loss):
       if self.config.resample_msa_in_recycling:
-        num_ensemble = batch_size // (self.config.num_recycle + 1)
+        num_ensemble = batch_size
         def slice_recycle_idx(x):
           start = recycle_idx * num_ensemble
           size = num_ensemble
@@ -341,53 +341,16 @@
           compute_loss=compute_loss,
           ensemble_representations=ensemble_representations)
 
-    if self.config.num_recycle:
-      emb_config = self.config.embeddings_and_evoformer
-      prev = {
-          'prev_pos': jnp.zeros(
-              [num_residues, residue_constants.atom_type_num, 3]),
-          'prev_msa_first_row': jnp.zeros(
-              [num_residues, emb_config.msa_channel]),
-          'prev_pair': jnp.zeros(
-              [num_residues, num_residues, emb_config.pair_channel]),
-      }
-
-      if 'num_iter_recycling' in batch:
-        # Training time: num_iter_recycling is in batch.
-        # The value for each ensemble batch is the same, so arbitrarily taking
-        # 0-th.
-        num_iter = batch['num_iter_recycling'][0]
-
-        # Add insurance that we will not run more
-        # recyclings than the model is configured to run.
-        num_iter = jnp.minimum(num_iter, self.config.num_recycle)
-      else:
-        # Eval mode or tests: use the maximum number of iterations.
-        num_iter = self.config.num_recycle
-
-      body = lambda x: (x[0] + 1,  # pylint: disable=g-long-lambda
-                        get_prev(do_call(x[1], recycle_idx=x[0],
-                                         compute_loss=False)))
-      if hk.running_init():
-        # When initializing the Haiku module, run one iteration of the
-        # while_loop to initialize the Haiku modules used in `body`.
-        _, prev = body((0, prev))
-      else:
-        _, prev = hk.while_loop(
-            lambda x: x[0] < num_iter,
-            body,
-            (0, prev))
-    else:
-      prev = {}
-      num_iter = 0
+    emb_config = self.config.embeddings_and_evoformer
+    ret = do_call(prev=batch.pop("prev"), recycle_idx=0)
+    ret["prev"] = get_prev(ret)
 
-    ret = do_call(prev=prev, recycle_idx=num_iter)
     if compute_loss:
       ret = ret[0], [ret[1]]
 
     if not return_representations:
       del (ret[0] if compute_loss else ret)['representations']  # pytype: disable=unsupported-operands
-    return ret
+    return ret, (None, None)
 
 
 class TemplatePairStack(hk.Module):
@@ -1730,9 +1693,7 @@
                                           True,
                                           name='prev_msa_first_row_norm')(
                                               batch['prev_msa_first_row'])
-        msa_activations = jax.ops.index_add(msa_activations, 0,
-                                            prev_msa_first_row)
-
+        msa_activations = msa_activations.at[0].add(prev_msa_first_row)
       if 'prev_pair' in batch:
         pair_activations += hk.LayerNorm([-1],
                                          True,
